{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('models')\n",
    "from models.model_coattn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### MCAT Implementation ###\n",
    "###########################\n",
    "class MCAT_Surv(nn.Module):\n",
    "    def __init__(self, fusion='concat', omic_sizes=[100, 200, 300, 400, 500, 600], model_size_wsi: str='small', \n",
    "        model_size_omic: str='small', n_classes=4, dropout=0.25):\n",
    "        r\"\"\"\n",
    "        Multimodal Co-Attention Transformer (MCAT) Implementation.\n",
    "\n",
    "        Args:\n",
    "            fusion (str): Late fusion method (Choices: concat, bilinear, or None)\n",
    "            omic_sizes (List): List of sizes of genomic embeddings\n",
    "            model_size_wsi (str): Size of WSI encoder (Choices: small or large)\n",
    "            model_size_omic (str): Size of Genomic encoder (Choices: small or large)\n",
    "            dropout (float): Dropout rate\n",
    "            n_classes (int): Output shape of NN\n",
    "        \"\"\"\n",
    "        super(MCAT_Surv, self).__init__()\n",
    "        self.fusion = fusion\n",
    "        self.omic_sizes = omic_sizes\n",
    "        self.n_classes = n_classes\n",
    "        self.size_dict_WSI = {\"small\": [1024, 256, 256], \"big\": [1024, 512, 384]}\n",
    "        self.size_dict_omic = {'small': [256, 256], 'big': [1024, 1024, 1024, 256]}\n",
    "        #self.criterion = SupConLoss(temperature=0.7)\n",
    "        \n",
    "        ### FC Layer over WSI bag\n",
    "        size = self.size_dict_WSI[model_size_wsi]\n",
    "        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n",
    "        fc.append(nn.Dropout(0.25))\n",
    "        self.wsi_net = nn.Sequential(*fc)\n",
    "        \n",
    "        ### Constructing Genomic SNN\n",
    "        hidden = self.size_dict_omic[model_size_omic]\n",
    "        sig_networks = []\n",
    "        for input_dim in omic_sizes:\n",
    "            fc_omic = [SNN_Block(dim1=input_dim, dim2=hidden[0])]\n",
    "            for i, _ in enumerate(hidden[1:]):\n",
    "                fc_omic.append(SNN_Block(dim1=hidden[i], dim2=hidden[i+1], dropout=0.25))\n",
    "            sig_networks.append(nn.Sequential(*fc_omic))\n",
    "        self.sig_networks = nn.ModuleList(sig_networks)\n",
    "\n",
    "        ### Multihead Attention\n",
    "        self.coattn = MultiheadAttention(embed_dim=256, num_heads=1)\n",
    "\n",
    "        ### Path Transformer + Attention Head\n",
    "        path_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')\n",
    "        self.path_transformer = nn.TransformerEncoder(path_encoder_layer, num_layers=2)\n",
    "        self.path_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)\n",
    "        self.path_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])\n",
    "        \n",
    "        ### Omic Transformer + Attention Head\n",
    "        omic_encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, dropout=dropout, activation='relu')\n",
    "        self.omic_transformer = nn.TransformerEncoder(omic_encoder_layer, num_layers=2)\n",
    "        self.omic_attention_head = Attn_Net_Gated(L=size[2], D=size[2], dropout=dropout, n_classes=1)\n",
    "        self.omic_rho = nn.Sequential(*[nn.Linear(size[2], size[2]), nn.ReLU(), nn.Dropout(dropout)])\n",
    "        \n",
    "        ### Fusion Layer\n",
    "        if self.fusion == 'concat':\n",
    "            self.mm = nn.Sequential(*[nn.Linear(256*2, size[2]), nn.ReLU(), nn.Linear(size[2], size[2]), nn.ReLU()])\n",
    "        elif self.fusion == 'bilinear':\n",
    "            self.mm = BilinearFusion(dim1=256, dim2=256, scale_dim1=8, scale_dim2=8, mmhid=256)\n",
    "        else:\n",
    "            self.mm = None\n",
    "        \n",
    "        ### Classifier\n",
    "        self.classifier = nn.Linear(size[2], n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x_path, x_omic):\n",
    "        ### Bag-Level Representation\n",
    "        print(\"*** 1. Bag-Level Representation (FC Processing) ***\")\n",
    "        h_path_bag = self.wsi_net(x_path).unsqueeze(1) ### path embeddings are fed through a FC layer\n",
    "        h_omic = [self.sig_networks[idx].forward(sig_feat) for idx, sig_feat in enumerate(x_omic)] ### each omic signature goes through it's own FC layer\n",
    "        h_omic_bag = torch.stack(h_omic).unsqueeze(1) ### omic embeddings are stacked (to be used in co-attention)\n",
    "        print(\"Instance-Level 256 x 256 Patch Embedings (H_bag before GCA):\\n\", h_path_bag.shape)\n",
    "        print(\"Genomic Embeddings (G_bag before GCA):\\n\", h_omic_bag.shape)\n",
    "        print()\n",
    "        \n",
    "        ### Genomic-Guided Co-Attention\n",
    "        print(\"*** 2. Genomic-Guided Co-Attention ***\")\n",
    "        h_path_coattn, A_coattn = self.coattn(h_omic_bag, h_path_bag, h_path_bag)\n",
    "        print(\"Genomic-Guided WSI-Level Embeddings (H_bag after GCA becomes H_coattn):\\n\", h_path_coattn.shape)\n",
    "        print(\"Genomic Embeddings (G_bag after GCA stays same):\\n\", h_omic_bag.shape)\n",
    "        print(\"Co-Attention Matrix:\\n\", A_coattn[0,0,:,:].shape)\n",
    "        print('- Note that the # of embeddings in H_coattn goes from 15231 -> 6')\n",
    "        print()\n",
    "\n",
    "        ### Set-Based MIL Transformers\n",
    "        print(\"*** 3. Set-based MIL Transformers ***\")\n",
    "        h_path_trans = self.path_transformer(h_path_coattn)\n",
    "        h_omic_trans = self.omic_transformer(h_omic_bag)\n",
    "        print(\"H_coattn after Transformers:\\n\", h_path_trans.shape)\n",
    "        print(\"G_bag after Transformers:\\n\", h_omic_trans.shape)\n",
    "        print('- Note that attention is permutation-equivariant, so dimensions are the same')        \n",
    "        print()\n",
    "        \n",
    "        ### Global Attention Pooling\n",
    "        print(\"*** 4. Global Attention Pooling ***\")\n",
    "        A_path, h_path = self.path_attention_head(h_path_trans.squeeze(1))\n",
    "        A_path = torch.transpose(A_path, 1, 0)\n",
    "        h_path = torch.mm(F.softmax(A_path, dim=1) , h_path)\n",
    "        h_path = self.path_rho(h_path).squeeze()\n",
    "        print(\"Final WSI-Level Representation (h^L):\\n\", h_path.shape)\n",
    "        \n",
    "        A_omic, h_omic = self.omic_attention_head(h_omic_trans.squeeze(1))\n",
    "        A_omic = torch.transpose(A_omic, 1, 0)\n",
    "        h_omic = torch.mm(F.softmax(A_omic, dim=1) , h_omic)\n",
    "        h_omic = self.omic_rho(h_omic).squeeze()\n",
    "        print(\"Final Genomic Representation (g^L):\\n\", h_omic.shape)\n",
    "        print()\n",
    "        \n",
    "        ### Late Fusion\n",
    "        print(\"*** 5. Late Fusion ***\")\n",
    "        if self.fusion == 'bilinear':\n",
    "            h = self.mm(h_path.unsqueeze(dim=0), h_omic.unsqueeze(dim=0)).squeeze()\n",
    "        elif self.fusion == 'concat':\n",
    "            h = self.mm(torch.cat([h_path, h_omic], axis=0))\n",
    "        print(\"Final shared representation (h_final):\\n\", h.shape)\n",
    "        print()\n",
    "        \n",
    "        ### Survival Layer\n",
    "        logits = self.classifier(h).unsqueeze(0)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        hazards = torch.sigmoid(logits)\n",
    "        S = torch.cumprod(1 - hazards, dim=1)\n",
    "        \n",
    "        attention_scores = {'coattn': A_coattn, 'path': A_path, 'omic': A_omic}\n",
    "        return hazards, S, Y_hat, attention_scores, None# F.normalize(h_path_coattn, dim=2), F.normalize(h_omic_bag, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 1. Bag-Level Representation (FC Processing) ***\n",
      "Instance-Level 256 x 256 Patch Embedings (H_bag before GCA):\n",
      " torch.Size([15231, 1, 256])\n",
      "Genomic Embeddings (G_bag before GCA):\n",
      " torch.Size([6, 1, 256])\n",
      "\n",
      "*** 2. Genomic-Guided Co-Attention ***\n",
      "Genomic-Guided WSI-Level Embeddings (H_bag after GCA becomes H_coattn):\n",
      " torch.Size([6, 1, 256])\n",
      "Genomic Embeddings (G_bag after GCA stays same):\n",
      " torch.Size([6, 1, 256])\n",
      "Co-Attention Matrix:\n",
      " torch.Size([6, 15231])\n",
      "- Note that the # of embeddings in H_coattn goes from 15231 -> 6\n",
      "\n",
      "*** 3. Set-based MIL Transformers ***\n",
      "H_coattn after Transformers:\n",
      " torch.Size([6, 1, 256])\n",
      "G_bag after Transformers:\n",
      " torch.Size([6, 1, 256])\n",
      "- Note that attention is permutation-equivariant, so dimensions are the same\n",
      "\n",
      "*** 4. Global Attention Pooling ***\n",
      "Final WSI-Level Representation (h^L):\n",
      " torch.Size([256])\n",
      "Final Genomic Representation (g^L):\n",
      " torch.Size([256])\n",
      "\n",
      "*** 5. Late Fusion ***\n",
      "Final shared representation (h_final):\n",
      " torch.Size([256])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5072, 0.5105, 0.5107, 0.4955]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4928, 0.2412, 0.1180, 0.0595]], grad_fn=<CumprodBackward>),\n",
       " tensor([[2]]),\n",
       " {'coattn': tensor([[[[-5.2956e-02, -2.7924e-02,  6.8047e-03,  ..., -1.4772e-01,\n",
       "             -2.3292e-01,  4.3942e-02],\n",
       "            [ 8.3089e-02, -4.5703e-02,  2.0750e-01,  ...,  9.3415e-02,\n",
       "             -1.7095e-01, -2.6080e-01],\n",
       "            [ 2.6508e-01,  3.9206e-01,  3.2596e-01,  ...,  2.1387e-01,\n",
       "              1.9855e-01,  2.3128e-01],\n",
       "            [-1.3625e-01, -2.4742e-01, -2.6088e-01,  ..., -2.7371e-01,\n",
       "             -1.0058e-01, -2.1232e-01],\n",
       "            [ 2.7769e-01,  1.5632e-01,  3.0384e-01,  ...,  7.5612e-02,\n",
       "              3.0659e-02,  2.5983e-04],\n",
       "            [ 2.4861e-01,  6.0646e-02,  8.7220e-02,  ..., -6.3945e-02,\n",
       "              2.3488e-02,  1.9480e-01]]]], grad_fn=<ViewBackward>),\n",
       "  'path': tensor([[ 0.1641, -0.1381, -0.0578, -0.1118,  0.1523, -0.2283]],\n",
       "         grad_fn=<TransposeBackward0>),\n",
       "  'omic': tensor([[ 0.1459,  0.2276, -0.0608, -0.2990, -0.1140, -0.1812]],\n",
       "         grad_fn=<TransposeBackward0>)},\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MCAT_Surv(omic_sizes=[100, 200, 300, 400, 500, 600])\n",
    "x_path = torch.randn((15231, 1024)) # 15231 patches with 1024-dim embedding size\n",
    "x_omic = [torch.randn(dim) for dim in [100, 200, 300, 400, 500, 600]]\n",
    "model.forward(x_path, x_omic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
